{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "선형회귀 뜯어보기\n",
    "\n",
    "http://kkokkilkon.tistory.com/77\n",
    "https://godongyoung.github.io/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2018/01/20/ISL-linear-regression_ch3.html\n",
    "http://woowabros.github.io/study/2018/08/01/linear_regression_qr.html\n",
    "https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80\n",
    "\n",
    "\n",
    "보스턴 데이터셋은 집값을 예측하는 목적을 가지고 있습니다.\n",
    "\n",
    "    집값 = y 값(label)\n",
    "\n",
    "    나머지 컬럼들 = x값(feature)\n",
    "    \n",
    "\n",
    "선형회귀는 각 feature 들이 집값과 선형적인 관계가 있다고 가정합니다.\n",
    "    - 값이 5 인것과 10 인것의 차이는 집값에 2 배의 영향을 준다고 봅니다.\n",
    "    \n",
    "\n",
    "전체 feature 중 중요한 것을 중요한 만큼 가중치(weight)를 주어서\n",
    "결국 해당 데이터셋을 가장 잘 표현할 수 있는 가중치들을 찾는 것이\n",
    "머신러닝 / 딥러닝의 목표인데요\n",
    "\n",
    "\n",
    "이 과정을 다양한 방법으로 알아보겠습니다.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_epoch = 45226, best_w1 = 0.03983, best_w2 = 0.08767, best_w3 = 0.23196, best_w4 = 0.09865, best_w5 = 0.66291, best_w6 = 0.32858, best_w7 = 0.16737, best_w8 = 0.35077, best_w9 = 0.06596, best_w10 = 0.01310, best_w11 = 0.44724, best_w12 = 0.00367, best_w13 = 0.63251, best_b = 0.51736, best_error = 608.52196\n",
      "\n",
      "99999\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "데이터 셋 Feature의 가중치(weight) 값 찾는 방법 \n",
    "\n",
    "1 : 랜덤으로 대입해서 그 중 가장 점수가 잘 나오는 값을 선택한다.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "y = boston[\"target\"]\n",
    "\n",
    "data = pd.DataFrame(X, columns=boston[\"feature_names\"])\n",
    "data[\"MEDV\"] = y\n",
    "\n",
    "x1 = X[:, 0] # CRIM\n",
    "x2 = X[:, 1] # ZN\n",
    "x3 = X[:, 2] # INDUS\n",
    "x4 = X[:, 3] # CHAS\n",
    "x5 = X[:, 4] # NOX\n",
    "x6 = X[:, 5] # RM\n",
    "x7 = X[:, 6] # AGE\n",
    "x8 = X[:, 7] # DIS\n",
    "x9 = X[:, 8] # RAD\n",
    "x10 = X[:, 9] # TAX\n",
    "x11 = X[:, 10] # PTRATIO\n",
    "x12 = X[:, 11] # B\n",
    "x13 = X[:, 12] # LSTAT\n",
    "\n",
    "\n",
    "num_epoch = 100000\n",
    "\n",
    "best_error = np.inf\n",
    "best_epoch = None\n",
    "best_w1 = None\n",
    "best_w2 = None\n",
    "best_b = None\n",
    "\n",
    "for epoch in range(num_epoch): # 랜덤은 사용자가 넣은 범위 내에서만 움직이기때문에 범위가 매우 중요\n",
    "    w1 = np.random.uniform(low=0.0, high=1.0) # 최소부터 최대까지 중 임의의 부동소수점(float) 숫자를 리턴\n",
    "    w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w3 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w4 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w5 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w6 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w7 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w8 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w9 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w10 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w11 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w12 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w13 = np.random.uniform(low=0.0, high=1.0)\n",
    "    b = np.random.uniform(low=0.0, high=1.0) #bias 가 뭐지 ? => 선형회귀 상수. \n",
    "\n",
    "    y_predict = (x1 * w1 + x2 * w2 + x3 * w3 + x4 * w4 + x5 * w5 + x6 * w6 + x7 * w7 + \n",
    "                 x8 * w8 + x9 * w9 + x10 * w10 + x11 * w11 + x12 * w12 + x13 * w13 + b) # why b?\n",
    "    \n",
    "    er = y_predict - y\n",
    "    error = np.sqrt(((er) ** 2).sum()) # 분산수식 루트[()^2 + ()^2 + . ..]  => 에러값이 작은경우에만 반영\n",
    "    \n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_epoch = epoch\n",
    "        best_w1 = w1\n",
    "        best_w2 = w2\n",
    "        best_w3 = w3\n",
    "        best_w4 = w4\n",
    "        best_w5 = w5\n",
    "        best_w6 = w6\n",
    "        best_w7 = w7\n",
    "        best_w8 = w8\n",
    "        best_w9 = w9\n",
    "        best_w10 = w10\n",
    "        best_w11 = w11\n",
    "        best_w12 = w12\n",
    "        best_w13 = w13\n",
    "        best_b = b\n",
    "\n",
    "        print(\"best_epoch = {0:4}, best_w1 = {1:.5f}, best_w2 = {2:.5f}, best_w3 = {3:.5f}, best_w4 = {4:.5f}, best_w5 = {5:.5f}, best_w6 = {6:.5f}, best_w7 = {7:.5f}, best_w8 = {8:.5f}, best_w9 = {9:.5f}, best_w10 = {10:.5f}, best_w11 = {11:.5f}, best_w12 = {12:.5f}, best_w13 = {13:.5f}, best_b = {14:.5f}, best_error = {15:.5f}\".format(epoch, best_w1, best_w2, best_w3, best_w4, best_w5, best_w6, best_w7, best_w8, best_w9, best_w10, best_w11, best_w12, best_w13, best_b, best_error), end=\"\\r\")\n",
    "print(\"\\n\\n\" + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99900 w1 = -0.089, w2 = 0.106, w3 = -0.192, w4 = 0.865, w5 = -0.549, w6 = 0.813, w7 = 0.108, w8 = -0.293, w9 = 0.047, w10 = 0.001, w11 = 0.600, w12 = 0.027, w13 = -0.797, b = 0.565, error = 146.16214\n",
      " === \n",
      " === \n",
      " === \n",
      " === \n",
      " === \n",
      " === \n",
      " === \n",
      " === \n",
      " === \n",
      " === \n",
      "99999 w1 = -0.0890, w2 = 0.1062, w3 = -0.1916, w4 = 0.8651, w5 = -0.5488, w6 = 0.8129, w7 = 0.1079, w8 = -0.2930, w9 = 0.0467, w10 = 0.0014, w11 = 0.5999, w12 = 0.0273, w13 = -0.7969, b = 0.5647, error = 146.15634\r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "데이터 셋 Feature의 가중치(weight) 값 찾는 방법 \n",
    "\n",
    "2 : 경사하강법을 사용한다.\n",
    "    - 예측한 값과 원본의 차이에 해당하는 정도만큼 가중치를 업데이트한다.\n",
    "    - 얼마나 반영할지 learning_rate 로 조절한다.\n",
    "    - 작으면 수렴하기까지 오래 걸리고, 크면 발산한다.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "y = boston[\"target\"]\n",
    "\n",
    "data = pd.DataFrame(X, columns=boston[\"feature_names\"])\n",
    "data[\"MEDV\"] = y\n",
    "\n",
    "x1 = X[:, 0] # CRIM\n",
    "x2 = X[:, 1] # ZN\n",
    "x3 = X[:, 2] # INDUS\n",
    "x4 = X[:, 3] # CHAS\n",
    "x5 = X[:, 4] # NOX\n",
    "x6 = X[:, 5] # RM\n",
    "x7 = X[:, 6] # AGE\n",
    "x8 = X[:, 7] # DIS\n",
    "x9 = X[:, 8] # RAD\n",
    "x10 = X[:, 9] # TAX\n",
    "x11 = X[:, 10] # PTRATIO\n",
    "x12 = X[:, 11] # B\n",
    "x13 = X[:, 12] # LSTAT\n",
    "\n",
    "\n",
    "num_epoch = 100000\n",
    "learning_rate = 0.000001 # rate를 업데이트 하는 비율. 작으면 수렴하기까지 오래 걸리고, 크면 발산한다.\n",
    "\n",
    "                                \n",
    "# 이녀석은 업데이트를 하기때문에 시작지점은 중요하지 않음. \n",
    "w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w2 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w3 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w4 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w5 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w6 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w7 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w8 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w9 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w10 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w11 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w12 = np.random.uniform(low=-1.0, high=1.0)\n",
    "w13 = np.random.uniform(low=-1.0, high=1.0)\n",
    "b = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = (x1 * w1 + x2 * w2 + x3 * w3 + x4 * w4 + x5 * w5 + x6 * w6 + x7 * w7 + \n",
    "                 x8 * w8 + x9 * w9 + x10 * w10 + x11 * w11 + x12 * w12 + x13 * w13 + b)\n",
    "    er = y_predict - y\n",
    "    error = np.sqrt(((er) ** 2).sum())\n",
    "    \n",
    "    if error < 10: #마지노선. 더작으면 너무 오래걸린다. 오버피팅 될수도 있다.\n",
    "        break\n",
    "# U자형 2차함수 생각하고 => 오류의 극솟값을 찾아간다고 생각하면 좋다.\n",
    "# y와 y_predict가 array라서 mean을 해야 한다. 506 => 1개로 됨\n",
    "# learning_rate는 w를 업데이트 시키는 비율. 가중치를 더하거나 빼서        \n",
    "# 최적점을 찾아가는 용도. learning_rate가 너무 크면 에러값이 수렴이하지 않거나 발산한다.\n",
    "# 너무 낮으면 에러가 줄어드는 속도가 느려진다. epoch와 learning_rate를 같이 고려해 에러값을 낮추는 점을 탐색해야함 \n",
    "    w1 = w1 - learning_rate * ((y_predict - y) * x1).mean() \n",
    "    w2 = w2 - learning_rate * ((y_predict - y) * x2).mean() \n",
    "    w3 = w3 - learning_rate * ((y_predict - y) * x3).mean() \n",
    "    w4 = w4 - learning_rate * ((y_predict - y) * x4).mean() \n",
    "    w5 = w5 - learning_rate * ((y_predict - y) * x5).mean()\n",
    "    w6 = w6 - learning_rate * ((y_predict - y) * x6).mean()\n",
    "    w7 = w7 - learning_rate * ((y_predict - y) * x7).mean()\n",
    "    w8 = w8 - learning_rate * ((y_predict - y) * x8).mean()\n",
    "    w9 = w9 - learning_rate * ((y_predict - y) * x9).mean()\n",
    "    w10 = w10 - learning_rate * ((y_predict - y) * x10).mean()\n",
    "    w11 = w11 - learning_rate * ((y_predict - y) * x11).mean()\n",
    "    w12 = w12 - learning_rate * ((y_predict - y) * x12).mean()\n",
    "    w13 = w13 - learning_rate * ((y_predict - y) * x13).mean()\n",
    "    b = b - learning_rate * (y_predict - y).mean()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"{0:2} w1 = {1:.3f}, w2 = {2:.3f}, w3 = {3:.3f}, w4 = {4:.3f}, w5 = {5:.3f}, w6 = {6:.3f}, w7 = {7:.3f}, w8 = {8:.3f}, w9 = {9:.3f}, w10 = {10:.3f}, w11 = {11:.3f}, w12 = {12:.3f}, w13 = {13:.3f}, b = {14:.3f}, error = {15:.5f}\".format(epoch, w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, b, error), end=\"\\r\")\n",
    "\n",
    "print(\"\\n === \"* 10)        \n",
    "print(\"{0:2} w1 = {1:.4f}, w2 = {2:.4f}, w3 = {3:.4f}, w4 = {4:.4f}, w5 = {5:.4f}, w6 = {6:.4f}, w7 = {7:.4f}, w8 = {8:.4f}, w9 = {9:.4f}, w10 = {10:.4f}, w11 = {11:.4f}, w12 = {12:.4f}, w13 = {13:.4f}, b = {14:.4f}, error = {15:.5f}\".format(epoch, w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, b, error), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "작업 속도를 빠르게 올리는 방법\n",
    "\n",
    "1. : 데이터셋을 행렬로 바꾼다(벡터화)\n",
    "\n",
    "'''\n",
    "\n",
    "import pprint\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "y = boston[\"target\"]\n",
    "\n",
    "\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "data = pd.DataFrame(X, columns=boston[\"feature_names\"])\n",
    "\n",
    "\n",
    "x = data.values \n",
    "min_max_scaler = preprocessing.MinMaxScaler() # 최소값 최대값 기준으로 정규화 \n",
    "x_scaled = min_max_scaler.fit_transform(x) # fit_transform ? ? ? watch out!\n",
    "data = pd.DataFrame(x_scaled, columns=feature_names)\n",
    "\n",
    "\n",
    "data[\"MEDV\"] = y\n",
    "label_name = \"MEDV\"\n",
    "\n",
    "\n",
    "X_train = data[feature_names].values.reshape(506, 13)\n",
    "y_train = data[label_name].values.reshape(506, 1)\n",
    "\n",
    "\n",
    "num_epoch = 1000000\n",
    "learning_rate = 0.00005\n",
    "\n",
    "\n",
    "w = np.random.uniform(low=-1.0, high=1.0, size=(13, 1))\n",
    "b = np.random.uniform(low=-1.0, high=1.0, size=(1))\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = X_train.dot(w) + b\n",
    "    er = y_predict - y_train\n",
    "    error = np.sqrt(((er).T.dot(er)).sum() / 506)\n",
    "    if np.abs(error) < 5:\n",
    "        break\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"{0:2} error = {1:.5f}\".format(epoch, error), end=\"\\r\")\n",
    "        \n",
    "    w = w - learning_rate * X_train.T.dot(y_predict - y_train)\n",
    "    b = b - learning_rate * (y_predict - y_train).mean()\n",
    "\n",
    "    \n",
    "print(\"----\" * 10)\n",
    "print(\"{0:2} error = {1:.5f}, \\n w = \\n{2}\".format(epoch, error, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score = 0.17050\n",
      "Features sorted by their score:\n",
      "[(0.4457, 'RM'), (0.3646, 'LSTAT'), (0.0691, 'DIS'), (0.037, 'CRIM'), (0.0205, 'NOX'), (0.0153, 'TAX'), (0.0153, 'PTRATIO'), (0.0113, 'B'), (0.0108, 'AGE'), (0.0055, 'INDUS'), (0.0034, 'RAD'), (0.0009, 'ZN'), (0.0007, 'CHAS')]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "작업 속도를 빠르게 올리는 방법\n",
    "\n",
    "2. : API / 라이브러리 사용한다.\n",
    "\n",
    "'''\n",
    "\n",
    "import pprint\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "#from keras.utils import to_categorical\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "y = boston[\"target\"]\n",
    "\n",
    "\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "data = pd.DataFrame(X, columns=boston[\"feature_names\"])\n",
    "data[\"MEDV\"] = y\n",
    "\n",
    "\n",
    "train = data\n",
    "label_name = \"MEDV\"\n",
    "\n",
    "\n",
    "y_train = train[label_name]\n",
    "X_train = train[feature_names]\n",
    "\n",
    "\n",
    "def rmsle(predicted_values, actual_values):\n",
    "    \n",
    "    predicted_values = np.array(predicted_values)\n",
    "    actual_values = np.array(actual_values)\n",
    "\n",
    "    log_predict = np.log(predicted_values + 1)\n",
    "    log_actual = np.log(actual_values + 1)\n",
    "\n",
    "    difference = log_predict - log_actual\n",
    "    difference = np.square(difference)\n",
    "\n",
    "    mean_difference = difference.mean()\n",
    "\n",
    "    score = np.sqrt(mean_difference)  \n",
    "\n",
    "    return score\n",
    "\n",
    "rmsle_score = make_scorer(rmsle)\n",
    "\n",
    "\n",
    "# 모델안에 들어가는 수치를 미리 정의  => RF함수 돌릴때 변수를 파라미터에 대입하는 경우 빈번.\n",
    "depth_cal = int(len(feature_names) * 0.7)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 200, n_jobs = 7, random_state = 22, max_depth = depth_cal)    # 최대 깊이는 바로 위 변수를 대입\n",
    "# n_jobs는 자기 컴퓨터 cpu 스레드갯수-1\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train,\n",
    "                        cv=20, scoring = rmsle_score).mean()  # cv 섹션 20개로 진행한 뒤 .mean()값을 한게 score로 나온다.\n",
    "\n",
    "\n",
    "X = X_train\n",
    "Y = y_train\n",
    "model.fit(X, Y)\n",
    "names = data[feature_names]\n",
    "\n",
    "\n",
    "print(\"Score = {0:.5f}\".format(score))\n",
    "print (\"Features sorted by their score:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), model.feature_importances_), names), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=9,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=7,\n",
       "           oob_score=False, random_state=22, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "# max_depth=9\n",
    "\n",
    "# min_impurity_decrease=0.0, \n",
    "#min_impurity_split=None,\n",
    "\n",
    "# max_leaf_nodes=None,       /        min_samples_leaf=1,       /       min_weight_fraction_leaf=0.0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
